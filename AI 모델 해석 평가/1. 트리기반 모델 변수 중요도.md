# 트리기반 모델 변수 중요도

### 1. 변수 중요도
* 알고리즘 별 내부 규칙에 의해, 예측에 대한 변수 별 영향도를 측정한 값
* 성능이 낮은 모델에서 변수 중요도는 의미가 없다.
* 즉, 변수 중요도를 뽑기 위해 트리 모델을 만드는 것이 아니라 최적의 모델이 트리일때, 그 모델을 설명하기 위해 변수 중요도를 확인한다.

### 2. Information Gain
* = 정보 전달량, 정보 획득량
* 지니 불순도가 감소하는 정도
* 지니 불순도를 가장 낮추는 변수와 값을 찾아서 분할함
* 부모 불순도 - 자식 불순도 = 정보 이득  

### 3. MDI
* 평균 불순도 감소(Mean Decrease Impurity, MDI)


### 3. Decision Tree
* Tree 전체에 대해서, feature 별로 information gain의 가중 평균을 계산

### 4. Random Forest
* 개별 Tree의 MDI로부터 feature별로 Importance 평균 계산

### 5. XGB
* 변수 중요도를 계산하는 3가지 방법
* plot_importance 와 model.feature_importances_ 가 서로 다른 변수 (weight, gain)를 사용하기 때문에 결과가 다르게 나오는 점 주의!

**1. weight**
* 모델 전체에서 해당 feature가 split 될 때 사용한 횟수의 합
* plot_importance 에서의 기본값

```python
# importance_type = 'weight'
plot_importance(model)
plt.show()
```

**2. gain**
* feature 별 평균 importmation gain
* gain을 보통 기본으로 사용함
* model.feature_importances_ 의 기본값

```python
# importance_type = 'gain'
plot_importance(model, importance_type='gain')
plt.show()
```

**3. cover**
* feature가 split할 때 샘플 수의 평균

```python
# importance_type = 'cover'
plot_importance(model, importance_type='cover')
plt.show()
```

### 4. 사용법
**1. 함수**

```python
# 변수 중요도 plot
def plot_feature_importance(importance, names, topn = 'all'):
    feature_importance = np.array(importance)
    feature_names = np.array(names)

    data={'feature_names':feature_names,'feature_importance':feature_importance}
    fi_temp = pd.DataFrame(data)

    fi_temp.sort_values(by=['feature_importance'], ascending=False,inplace=True)
    fi_temp.reset_index(drop=True, inplace = True)

    if topn == 'all' :
        fi_df = fi_temp.copy()
    else :
        fi_df = fi_temp.iloc[:topn]

    plt.figure(figsize=(10,8))
    sns.barplot(x='feature_importance', y='feature_names', data = fi_df)

    plt.xlabel('importance')
    plt.ylabel('feature names')
    plt.grid()

    return fi_df
```

**2. 함수 사용**
```python
# 모델 생성 및 학습 후

# GridSearchCV 혹은 RandomSearchCV를 사용한 경우 아래 코드 사용
# best_model만 추출해서 저장
model = model.best_estimator_

# 모델의 변수 중요도 시각화
result = plot_feature_importance(model.feature_importances_, list(x),12)
```