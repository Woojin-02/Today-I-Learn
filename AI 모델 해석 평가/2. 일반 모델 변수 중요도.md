# 일반 모델 변수 중요도

### 1. Permutation Feature Importance, PFI
* Permutation = 순열
* Feature = 변수
* Importance = 중요도
* 회귀 -> R2, 분류 -> Accuracy
* Feature 하나의 데이터를 무작위로 섞을 때, model의 score가 얼마나 감소되는지로 계산
    * 예를 들어 x2라는 변수가 있을 때, x2의 데이터를 무작위로 섞으면 데이터의 의미가 퇴색된다. 이때 성능을 검사하고, 성능이 원본에서 얼마나 떨어졌는지 차이를 구함
* 성능이 원본에 비해 떨어지면 떨어질수록 해당 변수의 중요도가 높다는 의미
    * 만약 값이 음수로 나오면 역으로 변수의 데이터를 무작위로 섞었을 때의 성능이 좋지 않다는 의미이기 때문에, 모델의 성능을 올리는 방법을 우선 고려할 필요가 있음
* 만약 **다중공선성**이 있는 변수가 있으면 특정 변수 하나가 있어도 나머지 다른 관련된 변수는 그대로 있으므로 score가 변하지 않고, 중요도를 제대로 측정할 수 없게 됨

### 2. 사용하면 좋은 함수

```python
# 변수 중요도 plot1
def plot_feature_importance(importance, names, topn = 'all'):
    feature_importance = np.array(importance)
    feature_names = np.array(names)

    data={'feature_names':feature_names,'feature_importance':feature_importance}
    fi_temp = pd.DataFrame(data)

    fi_temp.sort_values(by=['feature_importance'], ascending=False,inplace=True)
    fi_temp.reset_index(drop=True, inplace = True)

    if topn == 'all' :
        fi_df = fi_temp.copy()
    else :
        fi_df = fi_temp.iloc[:topn]

    plt.figure(figsize=(10,8))
    sns.barplot(x='feature_importance', y='feature_names', data = fi_df)

    plt.xlabel('importance')
    plt.ylabel('feature names')
    plt.grid()

    return fi_df
```

```python
# 변수 중요도 plot2
def plot_PFI(pfi, col_names):
    plt.figure(figsize = (14,5))
    plt.subplot(1,2,1)
    for i,vars in enumerate(col_names) :
        sns.kdeplot(pfi.importances[i], label = vars)
    plt.legend()
    plt.grid()

    sorted_idx = pfi.importances_mean.argsort()
    plt.subplot(1,2,2)
    plt.boxplot(pfi.importances[sorted_idx].T, vert=False, labels=col_names[sorted_idx])
    plt.axvline(0, color = 'r')
    plt.grid()
    plt.show()
```

### 3. 코드

```python
# 모델 학습 후 진행
pfi1 = permutation_importance(model1, x_val_s, y_val, n_repeats=10, scoring = 'r2', random_state=42)

# feature 별 score 분포
plot_PFI(pfi1, x.columns)

# 평균값으로 중요 분포도 변수 그리기
result = plot_feature_importance(pfi1.importances_mean, list(x_train))
```