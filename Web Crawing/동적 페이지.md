# 동적 웹 크롤링

1. 모듈 설치
```python
import requests  # 크롤링 모듈
import pandas as pd
import warnings
warnings.filterwarnings('ignore')
```

2. json
 : 동적 웹 크롤링은 json 문자을 이용하여 크롤링
 : 혹은 selenium 이용
 * selenium : 브라우지를 직접 열어서 데이터를 크롤링

### 기본 웹 크롤링
* 웹 크롤링 방법
    1. 크롤링하려는 웹으로 이동
    2. F12로 개발자모드 열기
    3. NetWork 탭 이동
    4. 크롤링하려는 데이터와 관련된 항목 클릭 -> 네트워크 트래픽 발생 확인 후 클릭
        * 클릭 후에 나오는 response 탭에서 원하는 데이터인지 확인 가능
    5. Headers 탭의 request URL 복사
        * 한글이 포함되어 있는 경우 `https://meyerweb.com/eric/tools/dencoder/`에 가서 디코딩 필요
* 웹 크롤링 코드
    * url : 웹 크롤링 주소
    * response = requests.get(url) : get 형식으로 url에 요청보내고, 그 응답을 response에 저장
    * response = requests.post(url, params, header) : post 형식으로 url에 요청보내고, 그 응답을 response에 저장
    * data = response.json() : response를 json 형식으로 가공
    * pd.DataFrame(data) : 데이터프레임 생성
* 웹 크롤링 함수
```python
def crawling1(page=1): 
    # 네이버증권 금시세 URL 
    url = f'https://m.stock.naver.com/front-api/v1/marketIndex/prices?page={page}&category=energy&reutersCode=CLcv1'
    # request(URL) -> response(JSON(str)) 
    response = requests.get(url)
    print(response) # 응답 확인(200:정상, 400:오류, 500:오류)
    # JSON(str) > list, dict > DataFrame
    data = response.json()
    return pd.DataFrame(data)
```

### 심화(차단당했을 때)
* url을 제대로 입력하고 다른 오류가 없음에도 '403 Forbidden 에러'가 발생한 경우

1. User-Agent 설정
* 가장 먼저 해야할 일
* User-Agent 설정 방법
    1. Headers 탭 가장 아래에 있는 User-Agent 복사
    2. 코드에 headers 설정(딕셔너리 형태로 작성)
```python
headers = {
    'User-Agent' : '복사한 user-agent 주소 붙여넣기',
}
response = requests.get(url, headers = headers)
response
```

* 그래도 오류가 나면
    * 개발자도구의 Headers 탭의 요소들을 하나씩 headers 딕셔너리에 넣으면서 테스트
    * 예를 들어, Daum은 User-Agent 이외에 Referer를 추가로 요구
    * 그 외에 Cookie를 요구하는 등 웹사이트 별로 각기 다른 것을 요구
    * 같은 탭의 Preview를 참고하면 도움이 될 수도 있음

* 예시 함수
```python
def crawling2(page=1): 
    # URL, header
    url = 'url 주소 입력'
    headers = {
        'User-Agent' : 'uer-agent 입력',
        'Referer' : 'referer 입력',
    }

    # request(URL) -> response(JSON(str)) 
    response = requests.get(url, headers = headers)
    # response

    # JSON(str) > list, dict > DataFrame
    data = response.json()['items'] # 데이터 중에서 필요한 데이터가 있는 것만 추출
    return pd.DataFrame(data)
```


### 공식 Rest Api
* 네이버, 카카오, 다음, 구글, 공공데이터포털 등 Rest Api를 제공해주는 곳들이 있음
* 사용 방법
    1. 회원가입 후 로그인
    2. 애플리케이션 등록 혹은 api 신청
    3. Request Token 얻기 -> app key 획득
    4. 제공처에 따라 필요한 api 키 사용 (자세한 사항은 공식 레퍼런스 및 구글 검색 참고)
        * 네이버는 client_id, client_secret 요구
        * 공공데이터포털은 Apikey만 요구
    5. 데이터 형식에 따라 parameter, headers 딕셔너리 생성(공식 레퍼런스 참고)
        * parameter에 한글이 있을 경우, 인코딩 필요(아래 코드 참고)
* 예제 함수
```python
import json
json.dumps(params)
```
```python
def crawling3(korean_text) : 
    #네이버의 경우
    client_id = ''
    client_secret = ''
    url = ''
    params = {딕셔너리 형태}
    headers = {
        'Content-Type': 'application/json', #파이썬에선 application/json을 사용
        'X-Naver-Client-Id': client_id,
        'X-Naver-Client-Secret': client_secret,
    }
    response = requests.post(url, json = params, headers=headers)
    return response.json()['응답1']['응답2']['응답3'] # 딕셔너리 형태의 응답 내부의 내부의 내부의 데이터 추출
```


### 참고
`크롤링한 데이터를 북석하려면 가설 수립, 전처리, 시각화, 데이터 스케일링이 필요함`
* 가설 수립
* 시각화 -> 만약 스케일이 달라 차이를 보기 힘들다면 데이터 스케일링
```python
import matplotlib.pyplot as plt 
import seaborn as sns
%config InlineBacked.figure_formats={'png', 'retina'}
```
* 데이터 스케일링 
```python
from sklearn.preprocessing import minmax_scale

minmax_scale(df['열이름'])
```
* 상관관계 분석
```python
df.corr()
```