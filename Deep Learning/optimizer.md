# Optimizer

* cost function에서 weight, bias를 이용하여 이러한 parameter를 어떤식으로 수정해 나갈것인지 결정하는 알고리즘

### 1. 경사하강법(Gradient Descent)
* 함수의 기울기(경사)를 구하여 함수의 극값에 이를 때까지 기우릭가 낮은 쪽으로 반복하여 이동하는 방법
* 기울기가 양수면(커지면) W를 작게 만든다.
* 기울기가 음수면 W를 크게 만들어야 한다.
* 알파를 조금 더했을 때 loss가 커지면 W를 작게 만들어야 한다
* SGD (batch 학습) 등이 있다.

### 2. 관성(Momentum)
* 이전에 이동했던 방향을 기억해서 다음 이동의 방향에 반영한다.
* 글로벌 미니멈 (Global Minimum : 목표)과 로컬 미니멈이 있는데, 경사 하강법으로는 로컬 미니멈을 해결할 수 없어서 글로벌 미니멈으로 도달 불가(중간에 다시 튀어올라가는 구간이 있기 때문에) 이 문제를 해결하기 위해 여러 해결 방안이 생겨남
    * 글로벌 미니엄(global minimum) : 정의역이 정해져있을 때, 최소가 되는 극값
    * 로컬 미니엄(local minimum) : 국소적 극값 (극소점, 극대점) 등
* 이전에 이동했던 방향을 기억해서 방향이 반대로 바뀌지 않고 원래 진행방향으로 갈 수 있도록 함
* NAG 등이 있음

### 3. Adagrad(Adaptive Gradient)
* 많이 이동한 변수(w)는 최적값에 근접했을 것이라는 가정하에, 많이 이동한 변수(w)를 기억해서 다음 이동의 거리를 줄인다.
* AdaGrad는 Feature별로 학습률(Learning rate)을 다르게 조절하는 것이 특징
* RMSprop 등이 있음
* 간혹 RMSprop를 사용함

### 4. Adam(RMSprop + Momentum)
* 보편적으로 사용됨

### 5. Vanishin Gradient
* 이전에는 Vanishing Gradient 문제 때문에 학습이 잘 되지 않았다.<br>
-> signoid가 어떤 차이가 나든 0~1 값만 출력되는 특징 때문에 loss가 변화가 없어서 weight를 조정할 수 없었음
앞단의 몇개는 조정이 되지만, 처음으로 돌아갈수록 조정 불가.