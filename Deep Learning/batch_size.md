# batch_size

```python
model.fit(독립, 종속, epochs=500, batch_size=64)
```
에서 사용함


* 딥러닝은 기본적으로 배치 단위로 학습하는데, 그와 관련된게 배치사이즈
* 배치 사이즈의 디폴트는 기본적으로 32라서, 데이터 수 / 32 = 결과값의 개수만큼 그룹이 나눠진다.
* fit 도중 나오는 5/5, 16/16이 바로 그 그룹의 개수
* Mini-batch gradient descent
* 배치 사이즈가 너무 작으면 target하고 완전히 다른 선이 뽑힐 가능성이 생김->로스가 많이 흔들림(증가했다 감소했다 왔다리갔다리)
* 배치 사이즈를 전체 데이터수와 같게 설정하면, 전체를 한꺼번에 잡고 타겟을 고정시킴
* 적절한 배치사이즈 찾는게 중요
* 배치 사이즈가 크면 weight를 조정받는 기회가 줄어들고, 작으면 로스가 올라가거나 흔들림
* 배치 사이즈가 클수록 학습되는 그룹 수가 작아짐
* batch 단위로 묶어서 학습을 하게 된다. 기본 사이즈는 32

    Epoch 1/500
    3/3 [==============================] - 0s 9ms/step - loss: 0.0074 - accuracy: 0.9933
    
    학습 결과 출력의 3/3 부분이 배치 사이즈를 나타냄. 현재 배치사이즈는 3

![](https://blog.kakaocdn.net/dn/k24n4/btrCHb6e0hy/ZkQjGQE8axo8C1iSj8lhDK/img.png)

### 트레이닝을 할 때에는 배치를 나누는데 추론할 때에는 왜 안나눌까?
</br>
학습에서는 가중치 변경이 일어나는데, 이 변경을 언제 하는지를 결정하는 것이 배치 사이즈. 추론에서는 가중치 변경이 일어나지 않기 때문에 배치를 나눌 필요가 없다.

